#!/bin/bash

#SBATCH --partition=gpu
#SBATCH --gpus=2
#SBATCH --job-name=FinetuneTFT
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --constraint=scratch-node
#SBATCH --time=06:00:00
#SBATCH --mem=64G

module load 2023
module load Python/3.11.3-GCCcore-12.3.0

cd $HOME/neuro-symbolic-demand-forecasting
poetry install
poetry shell

# Check whether the GPU is available
srun python -uc "import torch; print('GPU available?', torch.cuda.is_available())"

#Copy input file to scratch
mkdir -p "$TMPDIR"/data
cp $HOME/data/2023_05_cleaned_pv.csv "$TMPDIR"/data
cp $HOME/data/2023_06_cleaned_pv.csv "$TMPDIR"/data
cp $HOME/data/2023-04_to_08-amsterdam-actuals.csv "$TMPDIR"/data
cp $HOME/data/2023_weather_data_06_run_summer_from_04_to_08.csv "$TMPDIR"/data
cp $HOME/data/model_finetune_config.yaml "$TMPDIR"/data

srun python -m src.neuro_symbolic_demand_forecasting.main_finetune \
        -smd "$TMPDIR"/data/2023_05_cleaned_pv.csv,data/2023_06_cleaned_pv.csv \
        -wad "$TMPDIR"/data/2023-04_to_08-amsterdam-actuals.csv \
        -wfd "$TMPDIR"/data/2023_weather_data_06_run_summer_from_04_to_08.csv \
        -md "$TMPDIR"/data/model_finetune_config.yaml -sv $HOME/finetuning_baseline_tft

# cp # move model to $HOME/models/ directory
# run with sbatch training.job